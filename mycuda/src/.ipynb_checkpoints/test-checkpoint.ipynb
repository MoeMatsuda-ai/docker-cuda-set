{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep 21 07:26:20 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 50%   36C    P0    N/A /  30W |    388MiB /  2048MiB |      5%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu102\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(CustomSNN, self).__init__()\n",
    "    self.flatten = nn.Flatten() #Tensorの平坦化\n",
    "    self.linear_relu_stack = nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(6, 20),\n",
    "        nn.Linear(20, 3),\n",
    "        nn.Linear(3, 20),\n",
    "        nn.Linear(20, 6),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "      u = self.linear_relu_stack(x)\n",
    "      return u\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "size of x : <built-in method size of Tensor object at 0x7fc5a4cc9a90>\n",
      "size of x_ : <built-in method size of Tensor object at 0x7fc5967f0540>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# nn.ReLUの動作確認\n",
    "dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(dev)\n",
    "nnReLU = nn.ReLU()\n",
    "x = torch.rand(1, 28, 28, device=dev)\n",
    "x_ = nnReLU(x)\n",
    "print(f\"size of x : {x.size}\\nsize of x_ : {x_.size}\")\n",
    "print(x.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of y : <built-in method size of Tensor object at 0x7fc596804b30>\n",
      "y = tensor([0.4376, 0.5801, 0.7029, 0.7776, 0.3382])\n",
      "size of y_ : <built-in method size of Tensor object at 0x7fc596804b30>\n",
      "y_ = tensor([ 0.1658,  0.6706,  0.1917, -0.4989, -0.6103], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# nn.Linear の動作確認\n",
    "nnLinear = nn.Linear(5,5)\n",
    "y = torch.rand(5)\n",
    "y_ = nnLinear(y)\n",
    "print(f\"size of y : {y.size}\\ny = {y}\\nsize of y_ : {y.size}\\ny_ = {y_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu102\n",
      "cuda, True\n",
      "compute_61\n",
      "find gpu devices, 1\n",
      "cuda:0, NVIDIA GeForce GT 1030\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(f\"cuda, {torch.cuda.is_available()}\")\n",
    "print(f\"compute_{''.join(map(str,(torch.cuda.get_device_capability())))}\")\n",
    "device_num:int = torch.cuda.device_count()\n",
    "print(f\"find gpu devices, {device_num}\")\n",
    "for idx in range(device_num):\n",
    "    print(f\"cuda:{idx}, {torch.cuda.get_device_name(idx)}\")\n",
    "\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b46fb4dbf1c7bbf296eaaa1324f864c27ed5bce422b0b6282c3fd6cf82b450c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
